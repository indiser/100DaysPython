{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0e59977f9b584465bf6e0334c39dc943": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0ba3fc2c00d44e04aed5a74087000ffe",
              "IPY_MODEL_7c62cae17d8b4fd890d362bc1ad30251",
              "IPY_MODEL_22fde94968284426a7d9aa91a3aa67b6"
            ],
            "layout": "IPY_MODEL_5c42d54b847445c384e7ebdfea787b07"
          }
        },
        "0ba3fc2c00d44e04aed5a74087000ffe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d97682e2abc84962a9f1f13703968426",
            "placeholder": "​",
            "style": "IPY_MODEL_be3b772f406f4d02a921579ba756ca24",
            "value": "config.json: "
          }
        },
        "7c62cae17d8b4fd890d362bc1ad30251": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71ab2356c46e4634b9798218c5a7937c",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_11f6a81733094c54b3262b67a51180f3",
            "value": 1
          }
        },
        "22fde94968284426a7d9aa91a3aa67b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55556dc28680406cb6c5269bb5a7c3fe",
            "placeholder": "​",
            "style": "IPY_MODEL_b898e0b7d390423d9c3a670475d344e7",
            "value": " 2.35k/? [00:00&lt;00:00, 199kB/s]"
          }
        },
        "5c42d54b847445c384e7ebdfea787b07": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d97682e2abc84962a9f1f13703968426": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be3b772f406f4d02a921579ba756ca24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "71ab2356c46e4634b9798218c5a7937c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "11f6a81733094c54b3262b67a51180f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "55556dc28680406cb6c5269bb5a7c3fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b898e0b7d390423d9c3a670475d344e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c91c878b4a164963a80b018641ef5e02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f9fe984f41a740748d1ec25eaaaffb89",
              "IPY_MODEL_4028e97d397f4248859880895955d0e2",
              "IPY_MODEL_4949b086f02942a4bc82647dd8605a85"
            ],
            "layout": "IPY_MODEL_e45ee30415b3403b990f47cb599c90c9"
          }
        },
        "f9fe984f41a740748d1ec25eaaaffb89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a429bba0038426db7536b8fd8cc5d9a",
            "placeholder": "​",
            "style": "IPY_MODEL_dfee98b2fcc840e5850316ae3f903c94",
            "value": "kokoro-v1_0.pth: 100%"
          }
        },
        "4028e97d397f4248859880895955d0e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_481d53eb871a43e5963741b9e3e6c134",
            "max": 327212226,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_00a4dd6bbd1441c591666dd3ae7e79be",
            "value": 327212226
          }
        },
        "4949b086f02942a4bc82647dd8605a85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f53ea0731954f42b889e6c8b5f92ebc",
            "placeholder": "​",
            "style": "IPY_MODEL_72d23f4fa0b74011b5ecabaf5e4e4ab3",
            "value": " 327M/327M [00:01&lt;00:00, 188MB/s]"
          }
        },
        "e45ee30415b3403b990f47cb599c90c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a429bba0038426db7536b8fd8cc5d9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dfee98b2fcc840e5850316ae3f903c94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "481d53eb871a43e5963741b9e3e6c134": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00a4dd6bbd1441c591666dd3ae7e79be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8f53ea0731954f42b889e6c8b5f92ebc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72d23f4fa0b74011b5ecabaf5e4e4ab3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a84be3f271de451bb9429953cbba4f3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_da04692d12ae42b0bd41442ab3b5536e",
              "IPY_MODEL_0bfa8e2126b8485b9f3b82d742f62f5b",
              "IPY_MODEL_722eff5de0934a5ca90ed47b4e129fa4"
            ],
            "layout": "IPY_MODEL_3994a0aa66934c53b1f2137a051f30e9"
          }
        },
        "da04692d12ae42b0bd41442ab3b5536e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6516f5b34ff949ecb3103b153c7a158b",
            "placeholder": "​",
            "style": "IPY_MODEL_e8cb02a72bbf4904bb2a971f5a36cf7f",
            "value": "voices/af_heart.pt: 100%"
          }
        },
        "0bfa8e2126b8485b9f3b82d742f62f5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9288319ac9b14e028f0db81f9ffe8fea",
            "max": 523425,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6217f5367c534154a4c9cbd8e7fbf1f4",
            "value": 523425
          }
        },
        "722eff5de0934a5ca90ed47b4e129fa4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1391c0bf753f4605bf016d52a8356b5a",
            "placeholder": "​",
            "style": "IPY_MODEL_54965b27455f48c991ff217ba52e6cfe",
            "value": " 523k/523k [00:00&lt;00:00, 1.75MB/s]"
          }
        },
        "3994a0aa66934c53b1f2137a051f30e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6516f5b34ff949ecb3103b153c7a158b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8cb02a72bbf4904bb2a971f5a36cf7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9288319ac9b14e028f0db81f9ffe8fea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6217f5367c534154a4c9cbd8e7fbf1f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1391c0bf753f4605bf016d52a8356b5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54965b27455f48c991ff217ba52e6cfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pymupdf4llm\n",
        "!pip install mdclense"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-R_M1cTHtyrO",
        "outputId": "074213a6-3c93-4b0c-db37-d818eb777a49"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymupdf4llm\n",
            "  Downloading pymupdf4llm-0.2.9-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting pymupdf>=1.26.6 (from pymupdf4llm)\n",
            "  Downloading pymupdf-1.26.7-cp310-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from pymupdf4llm) (0.9.0)\n",
            "Downloading pymupdf4llm-0.2.9-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.3/72.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymupdf-1.26.7-cp310-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymupdf, pymupdf4llm\n",
            "Successfully installed pymupdf-1.26.7 pymupdf4llm-0.2.9\n",
            "Collecting mdclense\n",
            "  Downloading mdclense-0.1.2-py3-none-any.whl.metadata (3.6 kB)\n",
            "Downloading mdclense-0.1.2-py3-none-any.whl (5.1 kB)\n",
            "Installing collected packages: mdclense\n",
            "Successfully installed mdclense-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pymupdf4llm\n",
        "from mdclense.parser import MarkdownParser\n",
        "\n",
        "contents=pymupdf4llm.to_markdown(\n",
        "    doc=\"Self_improving_LLM.pdf\"\n",
        ")\n",
        "\n",
        "parser=MarkdownParser()\n",
        "text=parser.parse(contents)\n",
        "print(text)"
      ],
      "metadata": {
        "id": "8r7FhbKqzimi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5df0f978-2665-441b-9b09-4f32363a0450"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Consider using the pymupdf_layout package for a greatly improved page layout analysis.\n",
            "Self-Adapting Language Models\n",
            "\n",
            "Adam Zweiger [∗] Jyothish Pari [∗] [†] Han Guo Ekin Akyürek Yoon Kim Pulkit Agrawal [†]\n",
            "\n",
            "Massachusetts Institute of Technology\n",
            "\n",
            "[CODE BLOCK]\n",
            "\n",
            "Abstract\n",
            "\n",
            "Large language models (LLMs) are powerful but static; they lack mechanisms to\n",
            "adapt their weights in response to new tasks, knowledge, or examples. We introduce\n",
            "Se lf- A dapting L LMs (SEAL), a framework that enables LLMs to self-adapt by\n",
            "generating their own finetuning data and update directives. Given a new input,\n",
            "the model produces a self-edit —a generation that may restructure the information\n",
            "in different ways, specify optimization hyperparameters, or invoke tools for data\n",
            "augmentation and gradient-based updates. Through supervised finetuning (SFT),\n",
            "these self-edits result in persistent weight updates, enabling lasting adaptation. To\n",
            "train the model to produce effective self-edits, we use a reinforcement learning\n",
            "loop, using the downstream performance of the updated model as the reward signal.\n",
            "Unlike prior approaches that rely on separate adaptation modules or auxiliary\n",
            "networks, SEAL directly uses the model’s generation to parameterize and control\n",
            "its own adaptation process. Experiments on knowledge incorporation and fewshot generalization show that SEAL is a promising step toward language models\n",
            "capable of self-directed adaptation in response to new data. Our website and code\n",
            "is available at https://jyopari.github.io/posts/seal .\n",
            "\n",
            "1 Introduction\n",
            "\n",
            "Large language models (LLMs) pretrained on vast text corpora exhibit remarkable abilities in language\n",
            "understanding and generation [1, 2, 3, 4, 5]. However, adapting these powerful models for specific\n",
            "tasks [6], integrating new information [7], or mastering novel reasoning skills [8] remains challenging\n",
            "due to the limited availability of task-specific data. In this paper, we explore an intriguing hypothesis:\n",
            "can an LLM self-adapt by transforming or generating its own training data and learning procedure?\n",
            "\n",
            "As an analogy, consider a human student preparing for the final exam of a machine learning class.\n",
            "Many students rely on their notes to prepare for the exam. These notes are often derived from the\n",
            "lecture content, textbooks, or information available on the internet. Instead of relying on the raw\n",
            "content, assimilating and rewriting the information in the form of notes often improves the ability of\n",
            "students to understand the content and answer exam questions. This phenomenon of reinterpreting\n",
            "and augmenting external knowledge in a way that is easier to understand is not limited to just taking\n",
            "exams, but seems to be universally true of human learning across tasks. Furthermore, different\n",
            "humans assimilate information in different ways—some might condense the information into a visual\n",
            "diagram, some into text, or some might rely more on concrete mathematical descriptions.\n",
            "\n",
            "Such assimilation, restructuring, or rewriting of data as part of the learning process is in contrast\n",
            "with how large language models (LLMs) are typically trained and deployed. Given a new task,\n",
            "current LLMs consume and learn from the task data “as-is” via finetuning or in-context learning\n",
            "\n",
            "[9, 10, 11, 12]. However, such data may not be in an optimal format (or volume) for learning, and\n",
            "\n",
            "∗ Equal contribution.\n",
            "†Improbable AI Lab, CSAIL MIT\n",
            "\n",
            "39th Conference on Neural Information Processing Systems (NeurIPS 2025).\n",
            "\n",
            "Figure 1: Overview of SEAL. In each RL outer loop iteration, the model generates candidate\n",
            "self-edits (SE)—directives on how to update the weights—applies updates, evaluates performance on\n",
            "a downstream task, and uses the resulting rewards to improve the self-edit generation policy.\n",
            "\n",
            "current approaches do not enable models to develop bespoke strategies for how to best transform and\n",
            "learn from their training data.\n",
            "\n",
            "As a step towards better language model adaptation, we propose equipping LLMs with the ability to\n",
            "generate their own training data and finetuning directives in response to new inputs. In particular,\n",
            "we introduce a reinforcement learning algorithm that trains LLMs to generate “self-edits” —naturallanguage instructions that specify the data and, optionally, optimization hyperparameters for updating\n",
            "the model’s weights (see Figure 1). We refer to such models as Se lf- A dapting L LMs (SEAL).\n",
            "\n",
            "We evaluate SEAL on two applications. We first consider the task of integrating new factual\n",
            "knowledge into an LLM. Rather than finetuning directly on the passage text, we finetune on synthetic\n",
            "data generated by the SEAL model. Our results show that, following reinforcement learning (RL)\n",
            "training, finetuning on self-generated synthetic data improves question-answering performance on\n",
            "the no-passage-in-context variant of SQuAD [13] from 33.5% to 47.0%. Notably, self-generated data\n",
            "from SEAL outperforms synthetic data generated by GPT-4.1.\n",
            "\n",
            "We further evaluate SEAL on few-shot learning on a simplified subset of the ARC-AGI benchmark [14], where the model leverages a set of tools to autonomously select both synthetic data\n",
            "augmentations and optimization hyperparameters (e.g., learning rate, training epochs, selective loss\n",
            "computation over token types). Our experiments demonstrate that automatic selection and configuration of these tools using SEAL enhances performance compared to both standard in-context learning\n",
            "(ICL) and self-editing without RL training to use the tools effectively. These results collectively show\n",
            "that SEAL is a versatile framework for enabling language models to self-adapt.\n",
            "\n",
            "2 Related Work\n",
            "\n",
            "Synthetic Data Generation. The creation of synthetic data for LLM training is increasingly\n",
            "common, from large-scale pretraining datasets [15, 16, 17, 18, 19] to task-specific data augmentation\n",
            "\n",
            "[20, 21, 22] and instruction-tuning sets [23, 24]. For incorporation of a smaller corpus, Yang et al.\n",
            "\n",
            "[25] use synthetic data generation via graph-based prompting. SEAL builds on this line of work by\n",
            "using reinforcement learning to train a generative policy that directly maximizes the downstream\n",
            "utility of synthetic data when applied for gradient-based self-updates, rather than relying on static or\n",
            "heuristic generation strategies that are manually tuned.\n",
            "\n",
            "Knowledge Updating. Several recent works aim to modify or inject factual knowledge into\n",
            "language models via weight updates. Some methods attempt to directly locate specific parameters\n",
            "that correspond to individual facts [26, 27, 28]. Others propose generating additional finetuning\n",
            "data using the information in context [29, 30, 25, 31, 32]. We adopt the latter strategy, following\n",
            "Akyürek et al. [30], who propose generating logical implications of a fact and finetuning on them,\n",
            "and Lampinen et al. [31], who show that implication-based finetuning can even outperform in-context\n",
            "learning. We build on these approaches by training models through RL to generate more optimal\n",
            "finetuning data. Park et al. [32] show that prompting language models to generate question–answer\n",
            "(QA) pairs directly can outperform implication-style prompting. Because the SEAL framework is\n",
            "agnostic to the prompt and format of the self-edit data, it can also be trained to generate QA pairs or\n",
            "other output formats, as explored in §B.11.\n",
            "\n",
            "2\n",
            "\n",
            "Test-Time Training. Test-Time Training (TTT) temporarily adapts model weights based on the\n",
            "input the model receives [33, 34, 35, 36]. Akyürek et al. [36] show that combining TTT with ICL\n",
            "enables gradient-updates to outperform standard ICL in the few-shot setting. SEAL can be viewed\n",
            "as incorporating a round of TTT in its inner-loop optimization, leveraging TTT’s efficiency relative\n",
            "to full-scale training to perform multiple updates and reward the generated data that yields the\n",
            "greatest performance gain. Although our method is trained using single-example TTT episodes, we\n",
            "demonstrate in the knowledge incorporation setting that it generalizes to the continued pretraining\n",
            "(CPT) regime—where placing data directly in context is no longer feasible.\n",
            "\n",
            "Reinforcement Learning for LLMs. Reinforcement learning has played a central role in improving\n",
            "LLM behavior, originally through RLHF [37, 38]. More recently, RL with verifiable rewards has been\n",
            "applied to boost reasoning performance by optimizing the model directly for task success [39, 40, 41].\n",
            "SEAL applies RL not to optimize final answers or trace revisions, but to optimize the generation of\n",
            "self-edit data that is then used for weight updates.\n",
            "\n",
            "Meta-Learning and Self-Modifying Systems. SEAL embodies meta-learning principles [42, 43,\n",
            "44] by learning an adaptation strategy—how to generate effective self-edits—via its outer optimization\n",
            "loop. The goal is to learn how to learn efficiently from task contexts. In reinforcement learning,\n",
            "meta-learning has been used to train agents that learn new tasks quickly [45, 46, 47, 48]. Sun\n",
            "et al. [49] similarly apply RL to learn task-specific weight modulations, offering an alternative\n",
            "to LoRA finetuning that is orthogonal to our approach. A natural extension of meta-learning is\n",
            "self-referential networks, where models modify their own parameters [50, 51]. In the domain\n",
            "of large language models, recent work has applied meta-learning to improve LLM adaptation\n",
            "\n",
            "[52, 53, 54, 55, 49]. Notably, Hu et al. [53] train a smaller model to output token-specific weights\n",
            "during finetuning, addressing a knowledge incorporation task similar to ours, while Chen et al. [54]\n",
            "propose a hypernetwork that generates LoRA adapters conditioned on the input, enabling dynamic\n",
            "and task-specific parameterization. However, SEAL offers greater generality by leveraging the\n",
            "model’s existing generative capabilities to parametrize updates.\n",
            "\n",
            "Self-Improvement. Several recent works fall under the umbrella of self-improvement or selftraining. Methods such as RLAIF [56, 57] and self-rewarding language models [58, 59] use the\n",
            "model itself to provide reward signals, leveraging the observation that judging outputs is often easier\n",
            "than generating them [60]. Other recent works improve performance on mathematical tasks by\n",
            "using majority-vote or model confidence as reinforcement learning rewards, enabling performance\n",
            "improvement without access to ground-truth labels [61, 62, 63, 64, 65]. However, all of these methods\n",
            "are fundamentally limited by the model’s current evaluation abilities and self-consistency. In contrast,\n",
            "we view self-improvement through interaction with external data as a more powerful and scalable\n",
            "path. SEAL learns how to best utilize this external data for self-improvement.\n",
            "\n",
            "3 Methods\n",
            "\n",
            "We propose Self-Adapting LLMs (SEAL), a framework that enables language models to improve\n",
            "themselves by generating their own synthetic data and optimization parameters (“self-edits”) in response to new data. The model is trained to produce these self-edits directly through token generation\n",
            "with the data provided in the model’s context. Self-edit generation is learned via reinforcement\n",
            "learning (RL) where the model is rewarded for generating self-edits ( SE ) that, when applied, improve\n",
            "the model’s performance at the target task. SEAL can therefore be interpreted as an algorithm with\n",
            "two nested loops: an outer RL loop, which optimizes the self-edit generation, and an inner update\n",
            "loop, which uses the generated self-edit to update the model via gradient descent. Our method can be\n",
            "seen as an instance of meta-learning where we meta-learn how to generate effective self-edits.\n",
            "\n",
            "3.1 General Framework\n",
            "\n",
            "Let θ denote the parameters of the language model LM θ . SEAL operates on individual task instances\n",
            "( C, τ ) where C is a context containing information relevant to the task, and τ defines the downstream\n",
            "evaluation used to assess the model’s adaptation. For example, in knowledge incorporation, C is the\n",
            "passage intended to be integrated into the model’s internal knowledge, and τ is a set of questions and\n",
            "associated answers about the passage. In few-shot learning, C includes few-shot demonstrations of a\n",
            "\n",
            "3\n",
            "\n",
            "novel task, and τ is the query input and ground-truth output. Given C, the model generates a self-edit\n",
            "SE —the form of which varies by domain (see §3.2)—and updates its parameters via supervised\n",
            "finetuning: θ [′] ← SFT ( θ, SE ).\n",
            "\n",
            "We optimize the self-edit generation process using reinforcement learning: the model takes an action\n",
            "(generating SE ), receives a reward r based on LM θ′ ’s performance on τ, and updates its policy to\n",
            "maximize expected reward:\n",
            "L RL( θt ) := − E( C,τ ) ∼D E SE ∼ LM θt ( ·|C ) [ r ( SE , τ, θt )] . (1)\n",
            "\n",
            "Unlike in standard RL setups, the reward assigned to\n",
            "\n",
            "Algorithm 1\n",
            "\n",
            "a given action in our setting depends on the model\n",
            "parameters θ at the time the action is taken (since θ\n",
            "is updated to θ [′], which is then evaluated). As a re- 1: Input: LM θ, dataset D =\n",
            "\n",
            "2: for outer iteration t = 1 , 2\n",
            "\n",
            "sult, the underlying RL state must include the policy’s\n",
            "\n",
            "3: Sample ( C, τ ) ∼D\n",
            "\n",
            "parameters and is given by ( C, θ ), even though the\n",
            "\n",
            "4: Generate self-edit SE ∼\n",
            "\n",
            "policy’s observation is limited to C (placing θ directly 5: Inner Loop Update: θt [′]\n",
            "(state, action, reward) triples collected with a previousin context is infeasible). The implication of this is that 7:6: Evaluate:Compute reward: Ans ∼ r LM ←θt′\n",
            "version of the model, θ old, may become stale and mis- 8: Update: θt +1 ←\n",
            "aligned for the current model θ current. For this reason, 9: end for\n",
            "we adopt an on-policy approach, in which self-edits\n",
            "are sampled from—and, crucially, rewards are computed using—the current model.\n",
            "\n",
            "Algorithm 1 Self-Adapting LLMs (SEAL):\n",
            "Self-Edit Reinforcement Learning Loop\n",
            "\n",
            "1: Input: LM θ, dataset D = { ( C, τ ) }\n",
            "2: for outer iteration t = 1 , 2 , . . . do\n",
            "3: Sample ( C, τ ) ∼D\n",
            "4: Generate self-edit SE ∼ LM θ ( · | C )\n",
            "5: Inner Loop Update: θt [′] [←] [SFT] [(] [θ] t [,] [ SE] [)]\n",
            "6: Evaluate: Ans ∼ LM θt′ [(] · | [)]\n",
            "7: Compute reward: r ← r ( Ans , τ )\n",
            "8: Update: θt +1 ← RL_Update ( θt, r, SE )\n",
            "9: end for\n",
            "\n",
            "We experimented with various on-policy methods such as Group Relative Policy Optimization\n",
            "(GRPO) [66] and Proximal Policy Optimization (PPO) [67], but found the training to be unstable.\n",
            "Instead, we adopt ReST [EM] [40], a simpler approach based on filtered behavior cloning—also known\n",
            "as “rejection sampling + SFT” [68, 69, 38, 39, 70].\n",
            "\n",
            "ReST [EM] can be viewed as an expectation-maximization (EM) procedure: the E-step samples candidate outputs from the current model policy, and the M-step reinforces only those samples that receive\n",
            "positive reward through supervised finetuning. This approach optimizes an approximation of our\n",
            "objective (1) under the binary reward:\n",
            "\n",
            "r ( SE , τ, θt ) = �1 If on τ, adaptation using SE improves LM θt ’s performance [2] (2)\n",
            "\n",
            "0 Otherwise\n",
            "\n",
            "More precisely, in optimizing (1), we must compute the gradient ∇θtL RL. However, as we noted,\n",
            "the reward term r ( SE , τ, θt ) depends on θt in our setup but is not differentiable. We address this by\n",
            "treating the reward as fixed with respect to θt . With this approximation, the Monte-Carlo estimator\n",
            "for a minibatch of N contexts and M sampled self-edits per context becomes\n",
            "\n",
            "M\n",
            "rij ∇θt log pθt ( SE ij | Ci ) (3)\n",
            "\n",
            "j =1\n",
            "\n",
            "M\n",
            "rij\n",
            "\n",
            "j =1\n",
            "\n",
            "1\n",
            "∇θtL RL ≈− NM\n",
            "\n",
            "1\n",
            "= −\n",
            "NM\n",
            "\n",
            "N\n",
            "\n",
            "i =1\n",
            "\n",
            "N\n",
            "\n",
            "i =1\n",
            "\n",
            "T\n",
            "∇θt log pθt ( ys [(] [i,j] [)] | y<s [(] [i,j] [)] [, C] i [)] [,] (4)\n",
            "\n",
            "s =1\n",
            "\n",
            "where pθt denotes the model’s autoregressive distribution and ys [(] [i,j] [)] is the s [th] token of self-edit SE ij,\n",
            "the j [th] sample for context Ci . Since sequences with r = 0 can be ignored in (4), we have shown that\n",
            "ReST [EM], with simple “SFT on good self-edits,” indeed optimizes (1) under the binary reward (2)\n",
            "(with a stop-gradient applied to the reward term). The SEAL training loop is summarized in Alg. 1.\n",
            "\n",
            "Finally, we note that while the implementation in this work uses a single model for both generating\n",
            "self-edits and learning from these self-edits, it is also possible to decouple these roles. In such a\n",
            "“teacher-student” formulation [71], a student model would be updated using edits proposed by a\n",
            "separate teacher model. The teacher would then be trained via RL to generate edits that maximize\n",
            "student improvement.\n",
            "\n",
            "2The reward may also be assigned to the single self-edit that yields the greatest improvement among sampled\n",
            "candidates, which we do in knowledge incorporation, rather than to all edits that yield a positive improvement.\n",
            "\n",
            "4\n",
            "\n",
            "3.2 Domain Instantiations\n",
            "\n",
            "We instantiate the SEAL framework in two distinct domains: knowledge incorporation and few-shot\n",
            "learning. These domains were chosen to highlight two complementary forms of model adaptation:\n",
            "(1) the ability to integrate new information into a model’s weights so that it can be recalled without\n",
            "relying on context (evaluated using a no-context variant of SQuAD) and (2) the ability to generalize\n",
            "to novel tasks after seeing only a small number of examples (evaluated using ARC).\n",
            "\n",
            "Knowledge Incorporation. Our goal is to efficiently incorporate the information provided in a\n",
            "passage into the model’s weights. A promising recent approach involves using a language model\n",
            "to generate content derived from the passage, followed by finetuning on both the original passage\n",
            "and the generated content [29, 30, 25, 31, 32]. While the form of generated content may vary, we\n",
            "adopt what we consider the canonical format: implications derived from the passage . This approach,\n",
            "introduced in deductive closure training [30], converts a given context C into a set of implications\n",
            "SE = {s 1 , s 2 , . . ., sn} by prompting the model to “List several implications derived from the content.”\n",
            "The output may include inferences, logical consequences, or restatements of the original passage. In\n",
            "§B.11, we also explore alternative prompts such as “rewrite the passage in different ways” or “rewrite\n",
            "in a question-answer format” and show that our method improves performance by similar or greater\n",
            "margins regardless of the base prompt.\n",
            "\n",
            "Figure 2: Knowledge Incorporation Setup. Given a new passage, the model generates synthetic\n",
            "data (the self-edit ) in the form of “implications” of the passage. We then finetune on these outputs\n",
            "using LoRA. The updated model is evaluated on questions about the passage without access to the\n",
            "original text, and the resulting accuracy serves as the reward signal for reinforcement learning.\n",
            "\n",
            "These self-generated statements form the training data for a supervised finetuning (SFT) update: we\n",
            "compute the standard causal language-modeling loss over each sequence si and update the model\n",
            "parameters, yielding θ [′] . Since the amount of data per update is small and the number of updates we\n",
            "do in total is large, we use low-rank adapters (LoRA [72]) for efficient, lightweight tuning. Finally,\n",
            "the adapted model LM θ [′] is evaluated on the task τ . This process is shown in Figure 2.\n",
            "\n",
            "During RL training, the adapted model’s accuracy on τ defines the reward r that drives the outer\n",
            "RL optimization. This trains the model to restructure the passage in a way that is most effective for\n",
            "assimilation via finetuning.\n",
            "\n",
            "Figure 3: Few-Shot Learning with SEAL. Left: example ARC demonstrations. Center: the model\n",
            "generates a self-edit specifying augmentations and training hyperparameters. Right: the adapted\n",
            "model is evaluated on a held-out test input.\n",
            "\n",
            "5\n",
            "\n",
            "Few-Shot Learning. The Abstraction and Reasoning Corpus (ARC) [8] is a benchmark designed\n",
            "to test abstract reasoning and generalization from very limited examples. Each task includes a small\n",
            "set of input-output demonstrations and a held-out test input whose correct output must be predicted.\n",
            "\n",
            "We adopt the test-time training (TTT) protocol of Akyürek et al. [36], where augmentations of the\n",
            "few-shot examples are used to perform gradient-based adaptation. Rather than relying on manually\n",
            "tuned heuristics for selecting augmentations and optimization settings, we train SEAL to learn these\n",
            "decisions. This setting tests whether SEAL can autonomously configure the adaptation pipeline—\n",
            "determining which augmentations to apply and what optimization parameters to use.\n",
            "\n",
            "To implement this, we define a set of tools, each of which is a pre-defined function from Akyürek\n",
            "et al. [36] that transforms data or specifies training parameters. These include:\n",
            "Data augmentations: rotations, flips, reflections, transpositions, resizing operations (e.g., changing\n",
            "grid resolution), and chained or repeated transformations.\n",
            "Optimization parameters: learning rate, number of training epochs, and whether the loss is\n",
            "computed over all tokens or only output tokens.\n",
            "\n",
            "The model is prompted with a task’s few-shot demonstrations to generate a self-edit, which in this\n",
            "case is a specification of which tools to invoke and how to configure them, as shown in Figure 3. The\n",
            "self-edit is then applied to adapt the model via LoRA finetuning. The adapted model is evaluated on\n",
            "the held-out test input, and the result determines the reward for the self-edit generation.\n",
            "\n",
            "4 Results\n",
            "\n",
            "In this section we empirically evaluate SEAL across our two adaptation domains: few-shot learning\n",
            "and knowledge incorporation. Full training, hyperparameter, and evaluation details are provided in\n",
            "§A and §B.\n",
            "\n",
            "4.1 Few-Shot Learning\n",
            "\n",
            "We conduct our experiments using Llama-3.2-1B-Instruct, a small open-source model with\n",
            "no ARC-specific pretraining. Since most ARC tasks are challenging for models that have not\n",
            "been pretrained on ARC, we curate a subset of 11 tasks from the ARC training set and 8 from the\n",
            "evaluation set, filtered to ensure that they are solvable under optimal TTT configurations for a base\n",
            "Llama-3.2-1B-Instruct . While this is a small number of examples, note that Akyürek et al. [36]\n",
            "used the same TTT configuration for all tasks, and thus we do not need a large training set for learning\n",
            "a fixed self-edit. More details are included in §A.\n",
            "\n",
            "The model is trained using ReST [EM] by sampling 15 self-edits per training task. Each self-edit is\n",
            "applied individually to generate 15 updated models, which are then evaluated on the corresponding\n",
            "held-out test example. We reinforce only those self-edits that lead to correctly adapted models, i.e.,\n",
            "models that produce the correct output for the test input after adaptation.\n",
            "\n",
            "After training, we evaluate the model by generating 5 self-edits per held-out evaluation task and\n",
            "apply each one independently. We then report the percentage of self-edits that lead to correct outputs,\n",
            "yielding a success rate that reflects the quality of the learned self-edit generation policy.\n",
            "\n",
            "We compare against the following baselines:\n",
            "ICL (In-Context Learning): Llama-3.2-1B-Instruct is prompted with the given few-shot\n",
            "examples using Akyürek et al. [36]’s protocol, and directly queried on the test input.\n",
            "TTT + Self-Edit (w/o prior RL): Llama-3.2-1B-Instruct performs test-time training (TTT)\n",
            "using few-shot examples and synthetic augmentations, but without any prior RL to optimize which\n",
            "augmentations or training configurations to use.\n",
            "Oracle TTT: The model performs test-time training (TTT) using the optimal human-crafted\n",
            "configuration from Akyürek et al. [36]. This provides an upper bound of our method.\n",
            "\n",
            "We record results in Table 4.1. SEAL substantially improves adaptation success rate compared to\n",
            "baselines: 72.5% vs. 20% (with self-edits from the base model without RL training) and 0% (no adaptation), though performance remains below Oracle TTT, suggesting room for further improvement.\n",
            "\n",
            "6\n",
            "\n",
            "Method Success Rate (%)\n",
            "\n",
            "ICL 0\n",
            "TTT + Self-Edit (w/o prior RL) 20\n",
            "SEAL 72.5\n",
            "Oracle TTT 100\n",
            "\n",
            "Table 1: Few-shot Abstract Reasoning\n",
            "\n",
            "4.2 Knowledge Incorporation\n",
            "\n",
            "We experiment with Qwen2.5-7B on incorporating novel factual content from SQuAD passages\n",
            "\n",
            "[13]. We use the relatively simple SQuAD dataset because its passages can be fully “understood”\n",
            "by the base model in-context, yet the model cannot reliably answer questions about them without\n",
            "that context. We do 2 rounds of ReST [EM] with a batch of 50 contexts (see §B for further details). We\n",
            "compare SEAL on knowledge incorporation against the following baseline approaches:\n",
            "Base Model: The pretrained model is evaluated on downstream QA tasks without any adaptation\n",
            "or access to the passage.\n",
            "Train on Passage Only: The model is finetuned directly on the passage using the standard\n",
            "language modeling loss, without any synthetic data.\n",
            "Train on Passage + Synthetic Data: The model is trained on the passage along with self-generated\n",
            "implications. This is the same setup as SEAL but without any prior RL training.\n",
            "Train on Passage + GPT-4.1 Synthetic Data: The model is trained on the passage along with\n",
            "model-generated implications collected from GPT-4.1 via the OpenAI API.\n",
            "\n",
            "Table 4.2 reports mean no-context SQuAD accuracy under\n",
            "two regimes: single-passage updating (with LoRA), and\n",
            "small-scale continued pretraining (with full finetuning).\n",
            "We run continued pretraining (CPT) experiments with\n",
            "n = 200 documents, as well as the full SQuAD validation\n",
            "set of n = 2067 documents. In the single-passage setting,\n",
            "finetuning directly on the passage yields a negligible gain\n",
            "over the frozen base model (33.5% vs. 32.7%), confirming\n",
            "that using the raw data alone is insufficient. Augmenting\n",
            "with synthetic implications generated by GPT-4.1 boosts\n",
            "accuracy to 46.3%, an improvement of 12.8 percentage\n",
            "points over the passage-only baseline. Using synthetic\n",
            "data produced by the base Qwen-2.5-7B model yields\n",
            "39.7%, a 6.2-point increase. After reinforcement learning, SEAL further improves accuracy to 47.0%, notably\n",
            "outperforming using synthetic data from GPT-4.1, despite\n",
            "being a much smaller model.\n",
            "\n",
            "Figure 4: Accuracy over RL itera-\n",
            "tions. Each iteration consists of a minibatch of 50 contexts, each with 5 sampled self-edits. SEAL surpasses GPT4.1 synthetic data after two iterations of\n",
            "ReST [EM] on the no-context SQuAD set.\n",
            "\n",
            "In the CPT setting, the model assimilates information from\n",
            "\n",
            "batch of 50 contexts, each with 5 sam\n",
            "many passages in a single continued pretraining run. It\n",
            "\n",
            "pled self-edits. SEAL surpasses GPT\n",
            "is then evaluated on the union of all corresponding ques\n",
            "4.1 synthetic data after two iterations of\n",
            "\n",
            "tions. In this setting, we sample 5 self-edit generations\n",
            "\n",
            "ReST [EM] on the no-context SQuAD set.\n",
            "\n",
            "for each passage and take the aggregate synthetic dataset\n",
            "for continued pretraining. As shown in Table 4.2, we observe a similar ranking of methods as in\n",
            "the single-passage case, but with synthetic data from GPT-4.1 slightly outperforming SEAL. In the\n",
            "n = 200 setting, SEAL achieves an accuracy of 58.2%, exceeding its single-passage performance. We\n",
            "attribute this gain to the aggregation of multiple self-edit generations. Overall, the strong continued\n",
            "pretraining results of SEAL suggest that the self-editing policy generalizes beyond the original RL\n",
            "setup of creating synthetic data in a single generation for a single passage.\n",
            "\n",
            "Figure 4 tracks accuracy after each outer RL iteration. Two iterations suffice for SEAL to overtake\n",
            "GPT-4.1 data; subsequent iterations yield diminishing returns, suggesting that the policy quickly\n",
            "converges to an edit style that distills the passage into easily learnable atomic facts (see qualitative\n",
            "examples in Figure 5). All results use tuned hyperparameters (see §B).\n",
            "\n",
            "7\n",
            "\n",
            "Method Single Passage Continued Pretrain- Continued Pretraining\n",
            "(n = 1; LoRA) ing (n = 200; full-FT) (n = 2067; full-FT)\n",
            "\n",
            "Base model 32.7 32.7 29.0\n",
            "Train on Passage 33.5 36.0 31.2\n",
            "Train on Passage + Synthetic 39.7 50.6 43.4\n",
            "Train on Passage + GPT-4.1 Synthetic 46.3 59.4 49.2\n",
            "SEAL 47.0 58.2 46.4\n",
            "\n",
            "Table 2: Knowledge Incorporation Performance Across Passage Settings.\n",
            "\n",
            "Figure 5: Example Knowledge Incorporation Self-Edits Across RL Iterations. In this example,\n",
            "we see how RL leads to the generation of more detailed self-edits, which in turn results in better\n",
            "performance. While the progression is clear in this case, the differences across iterations are\n",
            "sometimes more subtle in other examples. We show in §B.11 that prompting for longer self-edits is\n",
            "effective, and that RL training further improves performance by a similar margin.\n",
            "\n",
            "5 Limitations\n",
            "\n",
            "Catastrophic forgetting. One key motivation we had for\n",
            "enabling language models to self-edit is to move towards\n",
            "the ultimate goal of continual learning—allowing models\n",
            "to incorporate new information over time, whether through\n",
            "agentically interacting with an environment or through\n",
            "standard training. While our earlier experiments assess\n",
            "how well SEAL adapts to individual edits in isolation,\n",
            "a more ambitious goal is to support sequences of edits:\n",
            "can the model adapt to new information repeatedly while\n",
            "preserving prior knowledge?\n",
            "\n",
            "This question relates directly to the challenge of catas-\n",
            "trophic forgetting [73, 74], where new updates interfere\n",
            "destructively with past learning. We do not explicitly optimize for retention in our current training setup, but we\n",
            "aim to establish a baseline for how well SEAL handles\n",
            "sequential self-edits without dedicated mechanisms for\n",
            "handling catastrophic forgetting. To test this, we simulate\n",
            "a continual learning setting in the knowledge incorporation domain. The model receives a stream of test passages,\n",
            "each triggering a new self-edit. After each update, we\n",
            "\n",
            "8\n",
            "\n",
            "Figure 6: Catastrophic forgetting from\n",
            "continual self-edits. We sequentially\n",
            "update the model on new passages and\n",
            "track degradation on prior tasks. Entrywise standard errors are reported in §B.6.\n",
            "\n",
            "re-evaluate the model on all previously seen tasks to measure retention. This setup tests the model’s\n",
            "ability to integrate new edits without forgetting earlier ones.\n",
            "\n",
            "As shown in Figure 6, performance on earlier tasks gradually declines as the number of edits increases,\n",
            "suggesting that SEAL is still susceptible to catastrophic forgetting. Still, it can perform multiple\n",
            "updates without complete collapse, indicating possibility for improvement. Future work could\n",
            "enhance this ability through reward shaping [75, 76, 77] to penalize regressions on earlier tasks, or by\n",
            "integrating continual learning strategies such as null-space constrained edits [78] or representational\n",
            "superposition [79]. In addition, since RL has been shown to forget less than SFT, SEAL’s inner loop\n",
            "could also employ RL instead of SFT [80].\n",
            "\n",
            "Computational overhead. The TTT reward loop is significantly more computationally expensive\n",
            "than other reinforcement learning loops used with LLMs. For instance, reward signals based on\n",
            "human preferences typically involve a single model forward pass, and those using verified solutions\n",
            "may rely on simple pattern matching (e.g., regex). In contrast, our approach requires finetuning and\n",
            "evaluating an entire model to compute the reward—each self-edit evaluation takes approximately\n",
            "30–45 seconds, introducing substantial overhead (see §B.5).\n",
            "\n",
            "Context-dependent evaluation. Our current instantiations assume that every context is paired with\n",
            "an explicit downstream task: few-shot demonstrations arrive with a held-out query pair, and each\n",
            "passage comes bundled with reference QA. This coupling simplifies reward computation but prevents\n",
            "RL training of SEAL from scaling to unlabeled corpora. A potential solution is to let the model\n",
            "generate not only self-edits but also its own evaluation questions—e.g., draft QA items or synthetic\n",
            "test cases for each passage—while the original content is still in context. These model-written\n",
            "queries could provide the immediate supervision required for reinforcement learning, broadening\n",
            "applicability to general training domains where external question-and-answer sets are unavailable.\n",
            "\n",
            "6 Discussion and Conclusion\n",
            "\n",
            "Villalobos et al. [81] project that frontier LLMs will be trained on all publicly available humangenerated text by 2028. We argue that this impending “data wall” will necessitate the adoption\n",
            "of synthetic data augmentation. Once web-scale corpora are exhausted, progress will hinge on a\n",
            "model’s capacity to generate its own high-utility training signal . A natural next step is to meta-train a\n",
            "dedicated SEAL synthetic-data generator model that produces fresh pretraining corpora, allowing\n",
            "future models to scale and achieve greater data efficiency without relying on additional human text.\n",
            "\n",
            "We can imagine a future in which LLMs can ingest new data, such as academic papers, and generate\n",
            "large quantities of explanations and implications for themselves using their existing knowledge and\n",
            "reasoning with the in-context data. This iterative loop of self-expression and self-refinement could\n",
            "allow models to keep improving on rare or underrepresented topics even in the absence of additional\n",
            "external supervision.\n",
            "\n",
            "In addition, while modern reasoning models are often trained with RL to generate chain-of-thought\n",
            "(CoT) traces, SEAL could offer a complementary mechanism, allowing the model to learn when and\n",
            "how to update its own weights. These two approaches could synergize: the model may choose to\n",
            "perform weight updates mid-reasoning to guide its current trajectory, or after completing reasoning\n",
            "to distill key insights into its parameters—improving future inference through internalized learning.\n",
            "\n",
            "This continual refinement loop is also promising for building agentic systems—models that operate\n",
            "over extended interactions and adapt dynamically to evolving goals. Agentic models must incrementally acquire and retain knowledge as they act. Our approach supports such behavior by enabling\n",
            "structured self-modification: after an interaction, the agent could synthesize a self-edit which triggers\n",
            "a weight update. This could allow the agent to develop over time, aligning its behavior with prior\n",
            "experience and reducing reliance on repeated supervision.\n",
            "\n",
            "SEAL demonstrates that large language models need not remain static after pretraining: by learning\n",
            "to generate their own synthetic self-edit data and to apply it through lightweight weight updates, they\n",
            "can autonomously incorporate new knowledge and adapt to novel tasks. Looking ahead, we envision\n",
            "extending the SEAL framework to pretraining, continual learning, and agentic models, ultimately\n",
            "enabling language models to self-learn and scale in a data-constrained world.\n",
            "\n",
            "9\n",
            "\n",
            "Acknowledgments and Disclosure of Funding\n",
            "\n",
            "We would like to thank Shivam Duggal, Idan Shenfeld, Seungwook Han, Jeremy Bernstein, Akarsh\n",
            "Kumar, Linlu Qiu, Juno Kim, Brian Cheung, Moritz Reuss, Ayush Sekhari, Zhang-Wei Hong,\n",
            "Mehul Damani, Leshem Choshen, and Ryan Yang for their valuable discussions and feedback. We\n",
            "acknowledge support from ARO MURI grant number W911NF-23-1-0277. This research was also\n",
            "partly sponsored by the United States Air Force Research Laboratory and the United States Air Force\n",
            "Artificial Intelligence Accelerator and was accomplished under Cooperative Agreement Number\n",
            "FA8750-19- 2-1000. The views and conclusions contained in this document are those of the authors\n",
            "and should not be interpreted as representing the official policies, either expressed or implied, of the\n",
            "United States Air Force or the U.S. Government. The U.S. Government is authorized to reproduce\n",
            "and distribute reprints for Government purposes, notwithstanding any copyright notation herein.\n",
            "We acknowledge the MIT Office of Research Computing and Data for providing high performance\n",
            "computing resources that have contributed to the research results reported within this paper. This\n",
            "research was also partly supported by the Stevens Fund for MIT UROP research and by the MIT-IBM\n",
            "Watson AI Lab.\n",
            "\n",
            "References\n",
            "\n",
            "[1] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child,\n",
            "Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse,\n",
            "Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark,\n",
            "Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.\n",
            "Language models are few-shot learners. In Advances in Neural Information Pro-\n",
            "cessing Systems, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/\n",
            "1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html .\n",
            "\n",
            "[2] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez,\n",
            "Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation\n",
            "language models, 2023. URL https://arxiv.org/abs/2302.13971 .\n",
            "\n",
            "[3] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian,\n",
            "Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The\n",
            "Llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783 .\n",
            "\n",
            "[4] Dirk Groeneveld, Iz Beltagy, Evan Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord,\n",
            "Ananya Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson,\n",
            "Russell Authur, Khyathi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack\n",
            "Hessel, Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik,\n",
            "Crystal Nam, Matthew Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk,\n",
            "Saurabh Shah, William Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep\n",
            "Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca\n",
            "Soldaini, Noah Smith, and Hannaneh Hajishirzi. OLMo: Accelerating the science of language\n",
            "models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd\n",
            "Annual Meeting of the Association for Computational Linguistics . Association for Computational Linguistics, 2024. URL https://aclanthology.org/2024.acl-long.841/ .\n",
            "\n",
            "[5] Qwen, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu,\n",
            "Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu,\n",
            "Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu,\n",
            "Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji\n",
            "Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang\n",
            "Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5\n",
            "technical report, 2025. URL https://arxiv.org/abs/2412.15115 .\n",
            "\n",
            "[6] Suchin Gururangan, Ana Marasovi´c, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\n",
            "and Noah A. Smith. Don’t stop pretraining: Adapt language models to domains and tasks. In\n",
            "\n",
            "10\n",
            "\n",
            "Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, Proceedings of the 58th\n",
            "Annual Meeting of the Association for Computational Linguistics . Association for Computational Linguistics, 2020. URL https://aclanthology.org/2020.acl-main.740/ .\n",
            "\n",
            "[7] Chen Zhu, Ankit Singh Rawat, Manzil Zaheer, Srinadh Bhojanapalli, Daliang Li, Felix Yu,\n",
            "and Sanjiv Kumar. Modifying memories in transformer models, 2020. URL https://arxiv.\n",
            "org/abs/2012.00363 .\n",
            "\n",
            "[8] Francois Chollet, Mike Knoop, Gregory Kamradt, and Bryan Landers. ARC prize 2024:\n",
            "Technical report, 2025. URL https://arxiv.org/abs/2412.04604 .\n",
            "\n",
            "[9] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan\n",
            "Du, Andrew M. Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In\n",
            "International Conference on Learning Representations, 2022. URL https://openreview.\n",
            "net/forum?id=gEZrGCozdqR .\n",
            "\n",
            "[10] Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan,\n",
            "Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov,\n",
            "Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan\n",
            "Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas\n",
            "Usunier, Thomas Scialom, and Gabriel Synnaeve. Code Llama: Open foundation models for\n",
            "code, 2024. URL https://arxiv.org/abs/2308.12950 .\n",
            "\n",
            "[11] Zeming Chen, Alejandro Hernández Cano, Angelika Romanou, Antoine Bonnet, Kyle Matoba,\n",
            "Francesco Salvi, Matteo Pagliardini, Simin Fan, Andreas Köpf, Amirkeivan Mohtashami,\n",
            "Alexandre Sallinen, Alireza Sakhaeirad, Vinitra Swamy, Igor Krawczuk, Deniz Bayazit, Axel\n",
            "Marmet, Syrielle Montariol, Mary-Anne Hartley, Martin Jaggi, and Antoine Bosselut. MediTron70B: Scaling medical pretraining for large language models, 2023. URL https://arxiv.\n",
            "org/abs/2311.16079 .\n",
            "\n",
            "[12] Pierre Colombo, Telmo Pessoa Pires, Malik Boudiaf, Dominic Culver, Rui Melo, Caio Corro,\n",
            "Andre F. T. Martins, Fabrizio Esposito, Vera Lúcia Raposo, Sofia Morgado, and Michael Desa.\n",
            "SaulLM-7B: A pioneering large language model for law, 2024. URL https://arxiv.org/\n",
            "abs/2403.03883 .\n",
            "\n",
            "[13] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+\n",
            "questions for machine comprehension of text. In Jian Su, Kevin Duh, and Xavier Carreras,\n",
            "editors, Proceedings of the 2016 Conference on Empirical Methods in Natural Language\n",
            "Processing . Association for Computational Linguistics, 2016. URL https://aclanthology.\n",
            "org/D16-1264/ .\n",
            "\n",
            "[14] François Chollet. On the measure of intelligence, 2019. URL https://arxiv.org/abs/\n",
            "1911.01547 .\n",
            "\n",
            "[15] Ronen Eldan and Yuanzhi Li. TinyStories: How small can language models be and still speak\n",
            "coherent English?, 2023. URL https://arxiv.org/abs/2305.07759 .\n",
            "\n",
            "[16] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio Cesar Teodoro Mendes, Allie Del Giorno,\n",
            "Sivakanth Gopi, Mojan Javaheripi, Piero Conti Kauffmann, Gustavo Henrique de Rosa, Olli\n",
            "Saarikivi, Adil Salim, Shital Shah, Harkirat Behl, Xin Wang, Sebastien Bubeck, Ronen Eldan,\n",
            "Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li. Textbooks are all you need, 2024. URL\n",
            "https://openreview.net/forum?id=Fq8tKtjACC .\n",
            "\n",
            "[17] Zeyuan Allen-Zhu and Yuanzhi Li. Physics of language models: Part 3.1, knowledge storage\n",
            "and extraction, 2024. URL https://arxiv.org/abs/2309.14316 .\n",
            "\n",
            "[18] Pratyush Maini, Skyler Seto, Richard Bai, David Grangier, Yizhe Zhang, and Navdeep Jaitly.\n",
            "Rephrasing the web: A recipe for compute and data-efficient language modeling. In Lun-Wei\n",
            "Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting\n",
            "of the Association for Computational Linguistics . Association for Computational Linguistics,\n",
            "URL https://aclanthology.org/2024.acl-long.757/ .\n",
            "\n",
            "11\n",
            "\n",
            "[19] Dan Su, Kezhi Kong, Ying Lin, Joseph Jennings, Brandon Norick, Markus Kliegl, Mostofa\n",
            "Patwary, Mohammad Shoeybi, and Bryan Catanzaro. Nemotron-CC: Transforming Common\n",
            "Crawl into a refined long-horizon pretraining dataset, 2025. URL https://arxiv.org/abs/\n",
            "2412.02595 .\n",
            "\n",
            "[20] Ruixiang Tang, Xiaotian Han, Xiaoqian Jiang, and Xia Hu. Does synthetic data generation of\n",
            "LLMs help clinical text mining?, 2023. URL https://arxiv.org/abs/2303.04360 .\n",
            "\n",
            "[21] Saumya Gandhi, Ritu Gala, Vijay Viswanathan, Tongshuang Wu, and Graham Neubig. Better\n",
            "synthetic data by retrieving and transforming existing datasets. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics .\n",
            "Association for Computational Linguistics, 2024. URL https://aclanthology.org/2024.\n",
            "findings-acl.385/ .\n",
            "\n",
            "[22] Yangjun Ruan, Neil Band, Chris J. Maddison, and Tatsunori Hashimoto. Reasoning to learn\n",
            "from latent thoughts, 2025. URL https://arxiv.org/abs/2503.18866 .\n",
            "\n",
            "[23] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi,\n",
            "and Hannaneh Hajishirzi. Self-Instruct: Aligning language models with self-generated instructions. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the\n",
            "61st Annual Meeting of the Association for Computational Linguistics . Association for Computational Linguistics, 2023. URL https://aclanthology.org/2023.acl-long.754/ .\n",
            "\n",
            "[24] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning\n",
            "with GPT-4, 2023. URL https://arxiv.org/abs/2304.03277 .\n",
            "\n",
            "[25] Zitong Yang, Neil Band, Shuangping Li, Emmanuel Candes, and Tatsunori Hashimoto. Synthetic\n",
            "continued pretraining. In The Thirteenth International Conference on Learning Representations,\n",
            "URL https://openreview.net/forum?id=07yvxWDSla .\n",
            "\n",
            "[26] Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher D Manning. Fast\n",
            "model editing at scale. In The Tenth International Conference on Learning Representations,\n",
            "URL https://openreview.net/forum?id=0DcZxeWfOPt .\n",
            "\n",
            "[27] Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations in GPT. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho,\n",
            "and A. Oh, editors, Advances in Neural Information Processing Systems . Curran Associates,\n",
            "Inc., 2022. URL https://proceedings.neurips.cc/paperfiles/paper/2022/file/\n",
            "6f1d43d5a82a37e89b0665b33bf3a182-Paper-Conference.pdf .\n",
            "\n",
            "[28] Kevin Meng, Arnab Sen Sharma, Alex J Andonian, Yonatan Belinkov, and David Bau. Massediting memory in a transformer. In The Eleventh International Conference on Learning\n",
            "Representations, 2023. URL https://openreview.net/forum?id=MkbcAHIYgyS .\n",
            "\n",
            "[29] Asaf Yehudai, Boaz Carmeli, Yosi Mass, Ofir Arviv, Nathaniel Mills, Eyal Shnarch, and Leshem\n",
            "Choshen. Achieving human parity in content-grounded datasets generation. In The Twelfth\n",
            "International Conference on Learning Representations, 2024. URL https://openreview.\n",
            "net/forum?id=RjYKTQ0L0W .\n",
            "\n",
            "[30] Afra Feyza Akyürek, Ekin Akyürek, Leshem Choshen, Derry Wijaya, and Jacob Andreas.\n",
            "Deductive closure training of language models for coherence, accuracy, and updatability. In\n",
            "Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for\n",
            "Computational Linguistics . Association for Computational Linguistics, 2024. URL https:\n",
            "//aclanthology.org/2024.findings-acl.584/ .\n",
            "\n",
            "[31] Andrew K. Lampinen, Arslan Chaudhry, Stephanie C. Y. Chan, Cody Wild, Diane Wan, Alex\n",
            "Ku, Jörg Bornschein, Razvan Pascanu, Murray Shanahan, and James L. McClelland. On the\n",
            "generalization of language models from in-context learning and finetuning: a controlled study,\n",
            "URL https://arxiv.org/abs/2505.00661 .\n",
            "\n",
            "[32] Core Francisco Park, Zechen Zhang, and Hidenori Tanaka. New News : System-2 fine-tuning for\n",
            "robust integration of new knowledge, 2025. URL https://arxiv.org/abs/2505.01812 .\n",
            "\n",
            "12\n",
            "\n",
            "[33] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei A. Efros, and Moritz Hardt. Testtime training with self-supervision for generalization under distribution shifts. In Proceedings\n",
            "of the 37th International Conference on Machine Learning . PMLR, 2020. URL http://\n",
            "proceedings.mlr.press/v119/sun20b.html .\n",
            "\n",
            "[34] Yossi Gandelsman, Yu Sun, Xinlei Chen, and Alexei Efros. Test-time training with\n",
            "masked autoencoders. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and\n",
            "A. Oh, editors, Advances in Neural Information Processing Systems . Curran Associates,\n",
            "Inc., 2022. URL https://proceedings.neurips.cc/paperfiles/paper/2022/file/\n",
            "bcdec1c2d60f94a93b6e36f937aa0530-Paper-Conference.pdf .\n",
            "\n",
            "[35] Yu Sun, Xinhao Li, Karan Dalal, Chloe Hsu, Sanmi Koyejo, Carlos Guestrin, Xiaolong Wang,\n",
            "Tatsunori Hashimoto, and Xinlei Chen. Learning to (learn at test time), 2024. URL https:\n",
            "//arxiv.org/abs/2310.13807 .\n",
            "\n",
            "[36] Ekin Akyürek, Mehul Damani, Adam Zweiger, Linlu Qiu, Han Guo, Jyothish Pari, Yoon Kim,\n",
            "and Jacob Andreas. The surprising effectiveness of test-time training for few-shot learning,\n",
            "URL https://arxiv.org/abs/2411.07279 .\n",
            "\n",
            "[37] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,\n",
            "Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton,\n",
            "Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with\n",
            "human feedback. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and\n",
            "A. Oh, editors, Advances in Neural Information Processing Systems . Curran Associates,\n",
            "Inc., 2022. URL https://proceedings.neurips.cc/paperfiles/paper/2022/file/\n",
            "b1efde53be364a73914f58805a001731-Paper-Conference.pdf .\n",
            "\n",
            "[38] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma,\n",
            "Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath,\n",
            "Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny\n",
            "Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine\n",
            "Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann,\n",
            "and Jared Kaplan. Training a helpful and harmless assistant with reinforcement learning from\n",
            "human feedback, 2022. URL https://arxiv.org/abs/2204.05862 .\n",
            "\n",
            "[39] Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. STaR: Bootstrapping reasoning with reasoning. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and\n",
            "A. Oh, editors, Advances in Neural Information Processing Systems . Curran Associates,\n",
            "Inc., 2022. URL https://proceedings.neurips.cc/paperfiles/paper/2022/file/\n",
            "639a9a172c044fbb64175b5fad42e9a5-Paper-Conference.pdf .\n",
            "\n",
            "[40] Avi Singh, John D Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Xavier Garcia,\n",
            "Peter J Liu, James Harrison, Jaehoon Lee, Kelvin Xu, Aaron T Parisi, Abhishek Kumar, Alexander A Alemi, Alex Rizkowsky, Azade Nova, Ben Adlam, Bernd Bohnet, Gamaleldin Fathy Elsayed, Hanie Sedghi, Igor Mordatch, Isabelle Simpson, Izzeddin Gur, Jasper Snoek, Jeffrey Pennington, Jiri Hron, Kathleen Kenealy, Kevin Swersky, Kshiteej Mahajan, Laura A Culp, Lechao\n",
            "Xiao, Maxwell Bileschi, Noah Constant, Roman Novak, Rosanne Liu, Tris Warkentin, Yamini\n",
            "Bansal, Ethan Dyer, Behnam Neyshabur, Jascha Sohl-Dickstein, and Noah Fiedel. Beyond human data: Scaling self-training for problem-solving with language models. Transactions on Ma-\n",
            "chine Learning Research, 2024. URL https://openreview.net/forum?id=lNAyUngGFK .\n",
            "\n",
            "[41] DeepSeek-AI. Deepseek-R1: Incentivizing reasoning capability in LLMs via reinforcement\n",
            "learning, 2025. URL https://arxiv.org/abs/2501.12948 .\n",
            "\n",
            "[42] Jürgen Schmidhuber. Evolutionary principles in self-referential learning, 1987. URL https:\n",
            "//people.idsia.ch/~juergen/diploma1987ocr.pdf .\n",
            "\n",
            "[43] Sepp Hochreiter, A. Steven Younger, and Peter R. Conwell. Learning to learn using gradient descent. In Georg Dorffner, Horst Bischof, and Kurt Hornik, editors, ICANN . Springer Berlin Heidelberg, 2001. URL https://link.springer.com/chapter/10.1007/3-540-44668-0\n",
            "13 .\n",
            "\n",
            "13\n",
            "\n",
            "[44] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th\n",
            "International Conference on Machine Learning, Proceedings of Machine Learning Research.\n",
            "PMLR, 2017. URL https://proceedings.mlr.press/v70/finn17a.html .\n",
            "\n",
            "[45] Yan Duan, John Schulman, Xi Chen, Peter L. Bartlett, Ilya Sutskever, and Pieter Abbeel. RL [2] :\n",
            "Fast reinforcement learning via slow reinforcement learning, 2016. URL https://arxiv.\n",
            "org/abs/1611.02779 .\n",
            "\n",
            "[46] Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos,\n",
            "Charles Blundell, Dharshan Kumaran, and Matt Botvinick. Learning to reinforcement learn,\n",
            "URL https://arxiv.org/abs/1611.05763 .\n",
            "\n",
            "[47] Kevin Frans, Jonathan Ho, Xi Chen, Pieter Abbeel, and John Schulman. Meta learning shared\n",
            "hierarchies. In The Sixth International Conference on Learning Representations, 2018. URL\n",
            "https://openreview.net/forum?id=SyX0IeWAW .\n",
            "\n",
            "[48] Abhishek Gupta, Russell Mendonca, YuXuan Liu, Pieter Abbeel, and Sergey Levine.\n",
            "Meta-reinforcement learning of structured exploration strategies. In S. Bengio,\n",
            "H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems . Curran Associates, Inc.,\n",
            "URL https://proceedings.neurips.cc/paperfiles/paper/2018/file/\n",
            "4de754248c196c85ee4fbdcee89179bd-Paper.pdf .\n",
            "\n",
            "[49] Qi Sun, Edoardo Cetin, and Yujin Tang. Transformer-Squared: Self-adaptive LLMs, 2025. URL\n",
            "https://arxiv.org/abs/2501.06252 .\n",
            "\n",
            "[50] Jurgen Schmidhuber. Steps towards ‘self-referential’ neural learning: A thought experiment,\n",
            "URL https://people.idsia.ch/~juergen/selfref1992.pdf .\n",
            "\n",
            "[51] Kazuki Irie, Imanol Schlag, Róbert Csordás, and Jürgen Schmidhuber. A modern self-referential\n",
            "weight matrix that learns to modify itself. In International Conference on Machine Learning .\n",
            "PMLR, 2022. URL https://proceedings.mlr.press/v162/irie22b.html .\n",
            "\n",
            "[52] Chenmien Tan, Ge Zhang, and Jie Fu. Massive editing for large language models via meta\n",
            "learning, 2024. URL https://arxiv.org/abs/2311.04661 .\n",
            "\n",
            "[53] Nathan Hu, Eric Mitchell, Christopher Manning, and Chelsea Finn. Meta-learning online\n",
            "adaptation of language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors,\n",
            "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing .\n",
            "Association for Computational Linguistics, 2023. URL https://aclanthology.org/2023.\n",
            "emnlp-main.268/ .\n",
            "\n",
            "[54] Tong Chen, Hao Fang, Patrick Xia, Xiaodong Liu, Benjamin Van Durme, Luke Zettlemoyer,\n",
            "Jianfeng Gao, and Hao Cheng. Generative Adapter: Contextualizing language models in\n",
            "parameters with a single forward pass. In The Thirteenth International Conference on Learning\n",
            "Representations, 2025. URL https://openreview.net/forum?id=bc3sUsS6ck .\n",
            "\n",
            "[55] Dan A. Calian, Gregory Farquhar, Iurii Kemaev, Luisa M. Zintgraf, Matteo Hessel, Jeremy\n",
            "Shar, Junhyuk Oh, András György, Tom Schaul, Jeffrey Dean, Hado van Hasselt, and David\n",
            "Silver. DataRater: Meta-learned dataset curation, 2025. URL https://arxiv.org/abs/\n",
            "2505.17895 .\n",
            "\n",
            "[56] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones,\n",
            "Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine\n",
            "Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli\n",
            "Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal\n",
            "Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer,\n",
            "Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston,\n",
            "Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton,\n",
            "Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben\n",
            "Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan.\n",
            "Constitutional AI: Harmlessness from AI feedback, 2022. URL https://arxiv.org/abs/\n",
            "2212.08073 .\n",
            "\n",
            "14\n",
            "\n",
            "[57] Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie Lu,\n",
            "Colton Bishop, Ethan Hall, Victor Carbune, Abhinav Rastogi, and Sushant Prakash. RLAIF vs.\n",
            "RLHF: Scaling reinforcement learning from human feedback with AI feedback. In Proceedings\n",
            "of the 41st International Conference on Machine Learning, ICML ’24. JMLR.org, 2024.\n",
            "\n",
            "[58] Jing-Cheng Pang, Pengyuan Wang, Kaiyuan Li, Xiong-Hui Chen, Jiacheng Xu, Zongzhang\n",
            "Zhang, and Yang Yu. Language model self-improvement by reinforcement learning contemplation. In The Twelfth International Conference on Learning Representations, 2024. URL\n",
            "https://openreview.net/forum?id=38E4yUbrgr .\n",
            "\n",
            "[59] Zhaoyang Wang, Weilei He, Zhiyuan Liang, Xuchao Zhang, Chetan Bansal, Ying Wei, Weitong\n",
            "Zhang, and Huaxiu Yao. CREAM: Consistency regularized self-rewarding language models.\n",
            "In The Thirteenth International Conference on Learning Representations, 2025. URL https:\n",
            "//openreview.net/forum?id=Vf6RDObyEF .\n",
            "\n",
            "[60] Yuda Song, Hanlin Zhang, Carson Eisenach, Sham M. Kakade, Dean Foster, and Udaya Ghai.\n",
            "Mind the gap: Examining the self-improvement capabilities of large language models. In\n",
            "The Thirteenth International Conference on Learning Representations, 2025. URL https:\n",
            "//openreview.net/forum?id=mtJSMcF3ek .\n",
            "\n",
            "[61] Jiaxin Huang, Shixiang Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei\n",
            "Han. Large language models can self-improve. In Houda Bouamor, Juan Pino, and Kalika\n",
            "Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language\n",
            "Processing . Association for Computational Linguistics, 2023. URL https://aclanthology.\n",
            "org/2023.emnlp-main.67/ .\n",
            "\n",
            "[62] Archiki Prasad, Weizhe Yuan, Richard Yuanzhe Pang, Jing Xu, Maryam Fazel-Zarandi, Mohit Bansal, Sainbayar Sukhbaatar, Jason Weston, and Jane Yu. Self-consistency preference\n",
            "optimization, 2024. URL https://arxiv.org/abs/2411.04109 .\n",
            "\n",
            "[63] Audrey Huang, Adam Block, Dylan J Foster, Dhruv Rohatgi, Cyril Zhang, Max Simchowitz,\n",
            "Jordan T. Ash, and Akshay Krishnamurthy. Self-improvement in language models: The sharpening mechanism. In The Thirteenth International Conference on Learning Representations,\n",
            "URL https://openreview.net/forum?id=WJaUkwci9o .\n",
            "\n",
            "[64] Yuxin Zuo, Kaiyan Zhang, Li Sheng, Shang Qu, Ganqu Cui, Xuekai Zhu, Haozhan Li, Yuchen\n",
            "Zhang, Xinwei Long, Ermo Hua, Biqing Qi, Youbang Sun, Zhiyuan Ma, Lifan Yuan, Ning Ding,\n",
            "and Bowen Zhou. TTRL: Test-time reinforcement learning, 2025. URL https://arxiv.org/\n",
            "abs/2504.16084 .\n",
            "\n",
            "[65] Sheikh Shafayat, Fahim Tajwar, Ruslan Salakhutdinov, Jeff Schneider, and Andrea Zanette.\n",
            "Can large reasoning models self-train?, 2025. URL https://arxiv.org/abs/2505.21444 .\n",
            "\n",
            "[66] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang,\n",
            "Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. DeepSeekMath: Pushing the limits of\n",
            "mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/\n",
            "2402.03300 .\n",
            "\n",
            "[67] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal\n",
            "policy optimization algorithms, 2017. URL https://arxiv.org/abs/1707.06347 .\n",
            "\n",
            "[68] W. R. Gilks and P. Wild. Adaptive rejection sampling for gibbs sampling. Journal of the Royal\n",
            "Statistical Society, 1992. URL http://www.jstor.org/stable/2347565 .\n",
            "\n",
            "[69] Aviral Kumar, Joey Hong, Anikait Singh, and Sergey Levine. When should we prefer offline\n",
            "reinforcement learning over behavioral cloning? In The Tenth International Conference on\n",
            "Learning Representations, 2022. URL https://openreview.net/forum?id=AP1MKT37rJ .\n",
            "\n",
            "[70] Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu, Chuanqi Tan, Chang\n",
            "Zhou, and Jingren Zhou. Scaling relationship on learning mathematical reasoning with large\n",
            "language models, 2023. URL https://arxiv.org/abs/2308.01825 .\n",
            "\n",
            "[71] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network,\n",
            "URL https://arxiv.org/abs/1503.02531 .\n",
            "\n",
            "15\n",
            "\n",
            "[72] Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,\n",
            "Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In\n",
            "The Tenth International Conference on Learning Representations, 2022. URL https:\n",
            "//openreview.net/forum?id=nZeVKeeFYf9 .\n",
            "\n",
            "[73] Michael McCloskey and Neal J. Cohen. Catastrophic interference in connectionist networks:\n",
            "The sequential learning problem, 1989. URL https://www.sciencedirect.com/science/\n",
            "article/pii/S0079742108605368 .\n",
            "\n",
            "[74] Ian J. Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, and Yoshua Bengio. An empirical\n",
            "investigation of catastrophic forgetting in gradient-based neural networks. In The Second\n",
            "International Conference on Learning Representations, 2014. URL https://openreview.\n",
            "net/forum?id=oXSw7laxwUpln .\n",
            "\n",
            "[75] Yujing Hu, Weixun Wang, Hangtian Jia, Yixiang Wang, Yingfeng Chen, Jianye Hao,\n",
            "Feng Wu, and Changjie Fan. Learning to utilize shaping rewards: A new approach\n",
            "of reward shaping. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and\n",
            "H. Lin, editors, Advances in Neural Information Processing Systems . Curran Associates,\n",
            "Inc., 2020. URL https://proceedings.neurips.cc/paperfiles/paper/2020/file/\n",
            "b710915795b9e9c02cf10d6d2bdb688c-Paper.pdf .\n",
            "\n",
            "[76] Tianbao Xie, Siheng Zhao, Chen Henry Wu, Yitao Liu, Qian Luo, Victor Zhong, Yanchao Yang,\n",
            "and Tao Yu. Text2Reward: Reward shaping with language models for reinforcement learning,\n",
            "URL https://arxiv.org/abs/2309.11489 .\n",
            "\n",
            "[77] Jiayi Fu, Xuandong Zhao, Chengyuan Yao, Heng Wang, Qi Han, and Yanghua Xiao. Reward\n",
            "shaping to mitigate reward hacking in RLHF, 2025. URL https://arxiv.org/abs/2502.\n",
            "18770 .\n",
            "\n",
            "[78] Junfeng Fang, Houcheng Jiang, Kun Wang, Yunshan Ma, Jie Shi, Xiang Wang, Xiangnan He,\n",
            "and Tat-Seng Chua. AlphaEdit: Null-space constrained model editing for language models.\n",
            "In The Thirteenth International Conference on Learning Representations, 2025. URL https:\n",
            "//openreview.net/forum?id=HvSytvg3Jh .\n",
            "\n",
            "[79] Brian Cheung, Alexander Terekhov, Yubei Chen, Pulkit Agrawal, and Bruno Olshausen. Superposition of many models into one. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc,\n",
            "E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems . Curran\n",
            "Associates, Inc., 2019. URL https://proceedings.neurips.cc/paperfiles/paper/\n",
            "2019/file/4c7a167bb329bd92580a99ce422d6fa6-Paper.pdf .\n",
            "\n",
            "[80] Idan Shenfeld, Jyothish Pari, and Pulkit Agrawal. Rl’s razor: Why online reinforcement learning\n",
            "forgets less, 2025. URL https://arxiv.org/abs/2509.04259 .\n",
            "\n",
            "[81] Pablo Villalobos, Anson Ho, Jaime Sevilla, Tamay Besiroglu, Lennart Heim, and Marius\n",
            "Hobbhahn. Will we run out of data? Limits of LLM scaling based on human-generated data,\n",
            "URL https://arxiv.org/abs/2211.04325 .\n",
            "\n",
            "[82] OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, et al. GPT-4 technical\n",
            "report, 2024. URL https://arxiv.org/abs/2303.08774 .\n",
            "\n",
            "[83] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. DeepSpeed: System\n",
            "optimizations enable training deep learning models with over 100 billion parameters. In\n",
            "Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery\n",
            "and Data Mining, KDD ’20. Association for Computing Machinery, 2020. URL https:\n",
            "//doi.org/10.1145/3394486.3406703 .\n",
            "\n",
            "[84] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu,\n",
            "Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large\n",
            "language model serving with PagedAttention. In Proceedings of the ACM SIGOPS 29th\n",
            "Symposium on Operating Systems Principles, 2023. URL https://dl.acm.org/doi/10.\n",
            "1145/3600006.3613165 .\n",
            "\n",
            "16\n",
            "\n",
            "[85] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh\n",
            "Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile\n",
            "Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut\n",
            "Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7B, 2023. URL\n",
            "https://arxiv.org/abs/2310.06825 .\n",
            "\n",
            "17\n",
            "\n",
            "A Experimental Details: Few-shot Learning\n",
            "\n",
            "A.1 Model and Setup\n",
            "\n",
            "For the few-shot learning experiments, we use Llama-3.2-1B-Instruct [3] as the base language\n",
            "model. Since this model has no specialized training on ARC, its ability to solve ARC tasks is limited.\n",
            "To enable controlled evaluation, we curated a small set of ARC problems from the training and\n",
            "evaluation splits that are solvable with optimal TTT hyperparameters.\n",
            "\n",
            "Training Set: We selected 11 ARC tasks from the training set as the environment for RL optimization.\n",
            "Evaluation Set: We selected 8 distinct ARC problems from the evaluation set for measuring\n",
            "generalization performance. These 8 were explicitly filtered for being amenable to TTT out of the\n",
            "full evaluation set.\n",
            "\n",
            "These sets were chosen to isolate the effect of self-edit learning rather than general ARC ability.\n",
            "\n",
            "A.2 Training Procedure\n",
            "\n",
            "We trained SEAL using a single outer loop of reinforcement learning on the 11 training problems.\n",
            "For each problem, the model generated 15 self-edit configurations consisting of:\n",
            "Data augmentation strategy: Controls whether to include variations such as basic examples, size-based augmentations, chained reasoning, and repeated inputs: includebasic,\n",
            "includesize, includechain, includerepeat .\n",
            "Optimization configuration: Specifies hyperparameters such as learning rate, number of\n",
            "epochs, and loss function type (e.g., whether to compute loss over all tokens or only the\n",
            "final output tokens).\n",
            "\n",
            "Each configuration was evaluated via test-time training (TTT), and assigned a binary reward: 1 if\n",
            "the adapted model produced the correct solution, 0 otherwise using Akyürek et al. [36]’s evaluation\n",
            "protocol. To encourage efficient adaptation, we discarded configurations requiring more than 375\n",
            "training steps, retained only configurations with reward 1 and acceptable cost for LoRA-based SFT.\n",
            "\n",
            "SFT for TTT was done with the following hyperparameters:\n",
            "\n",
            "LoRA adapters were inserted into the following modules: qproj, vproj, gateproj, downproj,\n",
            "[CODE BLOCK]\n",
            "\n",
            "For the RL SFT we used the following hyperparameters:\n",
            "\n",
            "When generating the self-edits, we add the following prompt as a suffix to the few-shot examples.\n",
            "\n",
            "18\n",
            "\n",
            "A.3 Evaluation Details\n",
            "\n",
            "For each of the 8 held-out evaluation tasks, the model generated 5 self-edit configurations, yielding\n",
            "a total of 40 configurations. Success was measured as the percentage of configurations that led to\n",
            "correct outputs after adaptation. We followed the evaluation protocol from Akyürek et al. [36].\n",
            "\n",
            "For the Oracle TTT we used the following configs:\n",
            "\n",
            "A.4 Compute Resources\n",
            "\n",
            "We performed all training runs on a single A100, H100, or H200. Each TTT per problem requires\n",
            "between half a minute to a few minutes, which is also why we limited the number of samples for\n",
            "ReST [EM] and additionally limited the number of gradient steps allowed per self-edit TTT. Overall\n",
            "ReST [EM] took around 2-3 hours.\n",
            "\n",
            "19\n",
            "\n",
            "B Experimental Details: Knowledge Incorporation\n",
            "\n",
            "B.1 Model and Setup\n",
            "\n",
            "We use the Qwen-2.5-7B base model [5] in the knowledge incorporation experiments. We repurpose\n",
            "the SQuAD dataset v1.1 [13] for the task of answering questions without the passage in-context.\n",
            "We use the training set for RL training and a 200-article subset of the evaluation set for evaluation.\n",
            "Within the training set and evaluation set, there are some overlapping topics of passages, but there\n",
            "is no overlap between these sets, so we can be sure that there is no data contamination of the test\n",
            "passages due to RL training.\n",
            "\n",
            "B.2 RL Training Procedure\n",
            "\n",
            "We run 2 rounds of ReST [EM] training [40]. On each round, we take a batch of 50 context-questionsanswers triples from the SQuAD training set. For each context, we sample 5 self-edit generations at\n",
            "temperature 1. We evaluate each self-edit over 3 random seeds, training on the sequences and then\n",
            "evaluating the updated model on the corresponding questions. We average each generation’s results\n",
            "over 3 seeds and then keep the single best generation for each of the 50 contexts. Finally, to finish the\n",
            "round of ReST [EM], we perform supervised finetuning on the 50 resulting prompt-completion pairs.\n",
            "\n",
            "Supervised finetuning here is done with batch size of 10, for 2 epochs, with learning rate 3e-4, using\n",
            "LoRA [72] with rank 64 and alpha 128, applied to all MLP and attention projection layers.\n",
            "\n",
            "B.3 Synthetic Data Generation and Finetuning Details\n",
            "\n",
            "In all models, we generate synthetic data by prompting to generate implications of the passage:\n",
            "\n",
            "We then take the resulting generated sequence. In the single-passage case, we split it by newlines\n",
            "into a set of training documents. In the multi-passage case, we use the full generated sequence as a\n",
            "single training document. In the case of synthetic data from GPT-4.1 ( gpt-4.1-2025-04-14 ), an\n",
            "instruct-model, we additionally have the following rule: If the second line begins with a “1.” then we\n",
            "omit the first line from the training set. This is because we found that the first line often contained\n",
            "filler text (e.g. “Sure, here is the list of implications:”).\n",
            "\n",
            "We then use the following training hyperparameters:\n",
            "\n",
            "Table 3: Single-Passage Knowledge Incorporation Hyperparameters\n",
            "\n",
            "Parameter Search Space\n",
            "\n",
            "LoRA Rank ( r ) [ 32, 64]\n",
            "LoRA Alpha ( α ) [32, 64 ]\n",
            "Learning Rate [1e-4, 3e-4, 5e-4, 1e-3, 2e-3]\n",
            "Epochs [1, 5, 10, 15, 20]\n",
            "Batch Size [ 1, 4]\n",
            "\n",
            "In the multi-passage n = 200 case, we sample 5 self-edit completions for each passage and take the\n",
            "aggregate dataset of all self-edits across all passages to train on.\n",
            "\n",
            "To answer the corresponding questions, we use the following prompt:\n",
            "\n",
            "20\n",
            "\n",
            "Table 4: Multi-Passage Knowledge Incorporation Hyperparameters\n",
            "\n",
            "Parameter Search Space\n",
            "\n",
            "LoRA Rank ( R ) [ 32, 64]\n",
            "LoRA Alpha ( α ) [32, 64 ]\n",
            "Learning Rate [1e-4, 3e-4, 5e-4, 1e-3, 2e-3]\n",
            "Epochs [1, 3, 5]\n",
            "Batch Size [1, 4, 8, 16]\n",
            "\n",
            "B.4 Evaluation Details\n",
            "\n",
            "We evaluate on a 200-passage subset of the SQuAD evaluation set, consisting of a combined 974\n",
            "evaluation questions (roughly 5 corresponding to each passage). The pipeline of generating synthetic\n",
            "data and finetuning on it is the same as above. For automated grading, we use gpt-4.1-2025-04-14\n",
            "\n",
            "[82] via the OpenAI API with greedy decoding.\n",
            "\n",
            "The grading prompt is as follows:\n",
            "\n",
            "B.5 Compute Resources\n",
            "\n",
            "All experiments are performed on 2 × H100 or 2 × H200. We use DeepSpeed ZeRO-3 [83] for SFT in\n",
            "ReST [EM] training. We use vLLM [84] for efficient inference. The most compute-intensive portion of\n",
            "our training and evaluation is the E-step of ReST [EM] training, where the model generates completions\n",
            "and is graded through the inner-loop process of finetuning and running inference. Doing a single\n",
            "round requires a batch of 50 passages over 5 completions and 3 runs per completion, meaning 750\n",
            "inner loop iterations. This takes about 6 hours on 2 × H100s.\n",
            "\n",
            "B.6 Standard Error of the Mean in Catastrophic Forgetting Experiment\n",
            "\n",
            "The standard errors of the mean (SEM) for each entry in Figure 6 is shown below in Table B.6.\n",
            "\n",
            "B.7 Scaling Model Size\n",
            "\n",
            "We further experimented with the 3B-parameter Qwen variant, with the same single-passage setup as\n",
            "in Figure 4. The results are given in Table 6.\n",
            "\n",
            "To compare the benefit of SEAL over using self-edits generated by the base model, we compute the\n",
            "ratio of SEAL’s improvement over the base model to the improvement from base model self-edits.\n",
            "This ratio is 1 . 75 × for the 3B model and 2 . 04 × for the 7B model. The relative improvement is greater\n",
            "for the 7B model, which provides some evidence that not only are stronger base models more effective\n",
            "at leveraging synthetic data for self-adaptation, but reinforcement learning may have compounding\n",
            "\n",
            "21\n",
            "\n",
            "Table 5: Entrywise standard errors of the mean (SEM) across continual self-edits experiment.\n",
            "\n",
            "1 2 3 4 5 6 7 8\n",
            "\n",
            "0 0.0306 0.0315 0.0263 0.0318 0.0297 0.0370 0.0310 0.0284\n",
            "1 0.0273 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
            "2 0.0305 0.0277 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
            "3 0.0277 0.0358 0.0406 0.0000 0.0000 0.0000 0.0000 0.0000\n",
            "4 0.0272 0.0303 0.0337 0.0320 0.0000 0.0000 0.0000 0.0000\n",
            "5 0.0296 0.0342 0.0290 0.0298 0.0319 0.0000 0.0000 0.0000\n",
            "6 0.0289 0.0334 0.0271 0.0258 0.0320 0.0337 0.0000 0.0000\n",
            "7 0.0255 0.0313 0.0264 0.0253 0.0309 0.0331 0.0363 0.0000\n",
            "8 0.0237 0.0307 0.0211 0.0267 0.0273 0.0271 0.0358 0.0263\n",
            "\n",
            "Table 6: Model Size Scaling Performance (%).\n",
            "\n",
            "Model Base Model (No Training) Base Model Self-Edit SEAL\n",
            "\n",
            "Qwen2.5-3B 25.1 31.9 37.0\n",
            "Qwen2.5-7B 32.7 39.7 47.0\n",
            "\n",
            "benefits as model capacity increases. We acknowledge that it is hard to draw conclusions though\n",
            "without actually scaling up further.\n",
            "\n",
            "B.8 Comparison to Generative Adapter\n",
            "\n",
            "We additionally compared with Generative Adapter [54], a hypernetwork approach that generates\n",
            "LoRA weights from context, using our evaluation setup. Table 7 reports results for both singlepassage ( n =1) and continued pretraining ( n = 200). We use the Mistral-7B-based model [85] for\n",
            "Generative Adapter, since that was the closest model for comparison. All values are on the same\n",
            "evaluation set, but CPT batches updates over all documents while single-passage trains and evaluates\n",
            "an adapter separately for each document. Generative Adapter achieves strong performance in the\n",
            "n =1 case, but underperforms SEAL in the CPT setting. SEAL’s parameterization of weight updates\n",
            "through synthetic data generation allows reuse of generated data for CPT, application to arbitrary\n",
            "base models, and flexibility to learn updates from diverse interaction types beyond LoRA finetuning.\n",
            "\n",
            "Table 7: SEAL vs. Generative Adapter Performance (%).\n",
            "\n",
            "Model Base Single-passage ( n =1) CPT ( n = 200)\n",
            "\n",
            "SEAL 32.0 47.0 58.2\n",
            "Generative Adapter 24.4 66.8 28.0\n",
            "\n",
            "We note that parameterizing weight updates via synthetic data generation rather than directly predicting LoRA weights has several advantages: (1) generated data can be reused for CPT or applied to\n",
            "arbitrary base models, (2) models can leverage reasoning and restructuring as document scale and\n",
            "complexity grow, and (3) the framework is not restricted to LoRA finetuning, allowing for many\n",
            "different update types, including those arising from environment or user interactions. By contrast,\n",
            "it is unclear how hypernetwork-based approaches would scale to such settings, while next-token\n",
            "prediction on generated data naturally exploits a model’s in-context learning capabilities.\n",
            "\n",
            "B.9 Comparison to Entigraph\n",
            "\n",
            "We additionally compare SEAL to Entigraph [25] in the Synthetic Continued Pretraining (SCPT)\n",
            "setting on SQuAD. Results for both 200 and 2067 passages are shown in Table 8. SEAL uses\n",
            "the same 5 synthetic data generations per document. For Entigraph, we sample 5 synthetic data\n",
            "generations involving pairs and 5 triplets of entities per document. Entigraph with all 10 synthetic\n",
            "data generations sampling is competitive with SEAL, especially at the larger scale. These results\n",
            "\n",
            "22\n",
            "\n",
            "suggest that RL-trained self-edits and structured heuristic methods are both strong approaches for\n",
            "synthetic data generation.\n",
            "\n",
            "Table 8: Synthetic Continued Pretraining (SCPT) on SQuAD (no passage in context). Best in each\n",
            "column is bolded.\n",
            "\n",
            "Method Continued Pretraining (n=200) Continued Pretraining (n=2067)\n",
            "\n",
            "SEAL 58.2 46.4\n",
            "Entigraph (pairs) 46.2 38.6\n",
            "Entigraph (pairs+triples) 56.0 48.6\n",
            "\n",
            "B.10 Proxy Reward\n",
            "\n",
            "We experiment with replacing the inner loop with a proxy reward based on a human-crafted rubric\n",
            "with 4 categories: length, diversity, quality, and correctness. A GPT-4.1 grader scores each category\n",
            "on a 1-5 scale, and the sum of these scores is used as the RL reward. Table 9 reports final results and\n",
            "RL training times.\n",
            "\n",
            "Table 9: Full Reward vs. Proxy Reward Performance (%).\n",
            "\n",
            "Model Base Post-RL Time\n",
            "\n",
            "SEAL 32.0 47.0 ≈ 6 hr\n",
            "SEAL w/ Proxy-Reward 32.0 45.6 ≈ 5 min\n",
            "\n",
            "While further tuning of the rubric or metric design could strengthen the reward signal, the advantage\n",
            "of the full SEAL loop is that no such manual specification is required—the model directly learns\n",
            "which edits improve its own performance. Both approaches appear promising for scaling to larger\n",
            "model sizes and compute budgets: proxy metrics offer dramatically lower cost, and with refinement,\n",
            "they may even surpass the “true” reward of directly optimizing for post-finetuning performance.\n",
            "\n",
            "B.11 Prompting\n",
            "\n",
            "Recent works have shown that reinforcement learning baselines and outcomes can be highly sensitive\n",
            "to prompting. We experiment with 6 additional self-edit prompts in the knowledge-incorporation\n",
            "setting. The seven prompts— implications, implications-long, implications-very-long,\n",
            "implications-chain-of-thought, rewrite, self-qa, and no-prompt —are shown below. All\n",
            "results in the main content of the paper used the implications prompt, which we consider to be the\n",
            "most prototypical [30, 31]. However, prior work has found prompts involving rewriting or generating\n",
            "question-answer pairs can be more effective, as discussed in §2.\n",
            "\n",
            "Furthermore, as we see qualitatively in Figure 5, RL appears to have dramatically increased the length\n",
            "of the response of the example. We therefore experiment with prompting for longer generations\n",
            "with implications-long and implications-very-long to test if we can achieve similar gains\n",
            "through prompting alone.\n",
            "\n",
            "The results are shown in Table 10. Notably, the baselines for implications-long and rewrite\n",
            "the RL-trained version of implications . However, using these prompts as the base of RL training\n",
            "yields even greater improvements. In all cases, ReST [EM] enhanced performance by roughly 6 to 11\n",
            "percentage points.\n",
            "\n",
            "Here, “Chain-of-thought-eval” refers to having the model reason before answering the questions\n",
            "(letting the model “pull out” information from its weights), rather than chain-of-thought before\n",
            "generating synthetic data, which is done with the base implications prompt. However, we did\n",
            "not notice a substantial difference in our setting when chain-of-thought was applied, whether before\n",
            "answering and before writing synthetic data.\n",
            "\n",
            "Letting the model “determine its own” self-edit format, with no-prompt, was not able to achieve\n",
            "the same results as predefined prompting formats in our experiments, achieving only 18 . 9% after 2\n",
            "rounds of training.\n",
            "\n",
            "23\n",
            "\n",
            "Method Original Round 1 Round 2 gpt-4.1 synthetic\n",
            "No self-edit 33.5    -    -    Implications 39.7 43.7 47.0 46.3\n",
            "Implications-long 49.3 52.4 54.4 54.1\n",
            "Implications-very-long 45.0 51.5 52.1 40.9\n",
            "Rewrite 49.4 55.3 55.6 54.4\n",
            "Self-QA 37.3 42.8 48.7 39.2\n",
            "No-Prompt 13.8 12.7 18.9 28.6\n",
            "Implications-chain-of-thought 38.7    -    -    Chain-of-thought-eval 37.8    -    -\n",
            "\n",
            "Table 10: Performance across 2 rounds of ReST [EM] RL training on various prompts in the singledocument knowledge incorporation setting. The gpt-4.1 column reports performance using synthetic\n",
            "data generated by gpt-4.1 with the corresponding prompt format.\n",
            "\n",
            "The five prompts are shown below.\n",
            "\n",
            "24\n",
            "\n",
            "Note: For self-qa, we apply additional formatting so that training documents consist of question–answer pairs, rather than using our standard approach of splitting by newline characters. Specifically, we split the output using occurrences of “Question n:” instead of newlines.\n",
            "\n",
            "25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q kokoro>=0.9.2 soundfile\n",
        "!apt-get -qq -y install espeak-ng > /dev/null 2>&1"
      ],
      "metadata": {
        "id": "H9Ye-5Hj6Zj6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import soundfile as sf\n",
        "import torch\n",
        "from kokoro import KPipeline\n",
        "\n",
        "# 1. Setup\n",
        "pipeline = KPipeline(lang_code='a')\n",
        "output_path = \"final_audiobook.wav\"\n",
        "\n",
        "# 2. The Streaming Loop\n",
        "print(f\"Stream-writing to {output_path}...\")\n",
        "\n",
        "# We open the file ONCE in write mode ('w')\n",
        "# We must specify samplerate and channels upfront because we aren't writing all data at once\n",
        "with sf.SoundFile(output_path, mode='w', samplerate=24000, channels=1) as file:\n",
        "    # Get the generator\n",
        "    generator = pipeline(text, voice='af_heart', speed=1)\n",
        "\n",
        "    for i, (gs, ps, audio) in enumerate(generator):\n",
        "        # Write the chunk directly to disk\n",
        "        file.write(audio)\n",
        "\n",
        "        # Optional: Print status every 10 chunks so you know it's alive\n",
        "        if i % 10 == 0:\n",
        "            print(f\"Saved chunk {i}...\", end='\\r')\n",
        "\n",
        "print(f\"\\nSuccess! Saved to {output_path}\")"
      ],
      "metadata": {
        "id": "GbNAXTzx6I5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "0e59977f9b584465bf6e0334c39dc943",
            "0ba3fc2c00d44e04aed5a74087000ffe",
            "7c62cae17d8b4fd890d362bc1ad30251",
            "22fde94968284426a7d9aa91a3aa67b6",
            "5c42d54b847445c384e7ebdfea787b07",
            "d97682e2abc84962a9f1f13703968426",
            "be3b772f406f4d02a921579ba756ca24",
            "71ab2356c46e4634b9798218c5a7937c",
            "11f6a81733094c54b3262b67a51180f3",
            "55556dc28680406cb6c5269bb5a7c3fe",
            "b898e0b7d390423d9c3a670475d344e7",
            "c91c878b4a164963a80b018641ef5e02",
            "f9fe984f41a740748d1ec25eaaaffb89",
            "4028e97d397f4248859880895955d0e2",
            "4949b086f02942a4bc82647dd8605a85",
            "e45ee30415b3403b990f47cb599c90c9",
            "0a429bba0038426db7536b8fd8cc5d9a",
            "dfee98b2fcc840e5850316ae3f903c94",
            "481d53eb871a43e5963741b9e3e6c134",
            "00a4dd6bbd1441c591666dd3ae7e79be",
            "8f53ea0731954f42b889e6c8b5f92ebc",
            "72d23f4fa0b74011b5ecabaf5e4e4ab3",
            "a84be3f271de451bb9429953cbba4f3a",
            "da04692d12ae42b0bd41442ab3b5536e",
            "0bfa8e2126b8485b9f3b82d742f62f5b",
            "722eff5de0934a5ca90ed47b4e129fa4",
            "3994a0aa66934c53b1f2137a051f30e9",
            "6516f5b34ff949ecb3103b153c7a158b",
            "e8cb02a72bbf4904bb2a971f5a36cf7f",
            "9288319ac9b14e028f0db81f9ffe8fea",
            "6217f5367c534154a4c9cbd8e7fbf1f4",
            "1391c0bf753f4605bf016d52a8356b5a",
            "54965b27455f48c991ff217ba52e6cfe"
          ]
        },
        "outputId": "91687839-b297-4c32-b2df-2da22ef9bf1f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: Defaulting repo_id to hexgrad/Kokoro-82M. Pass repo_id='hexgrad/Kokoro-82M' to suppress this warning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0e59977f9b584465bf6e0334c39dc943"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
            "  WeightNorm.apply(module, name, dim)\n",
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "kokoro-v1_0.pth:   0%|          | 0.00/327M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c91c878b4a164963a80b018641ef5e02"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stream-writing to final_audiobook.wav...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "voices/af_heart.pt:   0%|          | 0.00/523k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a84be3f271de451bb9429953cbba4f3a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 60...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 70...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 80...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 90...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 100...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 140...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 150...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 160...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 170...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 180...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 190...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 200...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 210...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 220...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 230...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 240...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 270...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 280...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 290...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 300...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 310...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 320...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 330...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 340...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 350...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 380...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 390...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 400...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 300.0% of the lines (3/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 460...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 300.0% of the lines (3/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 470...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 300.0% of the lines (3/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 300.0% of the lines (3/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 480...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 300.0% of the lines (3/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 300.0% of the lines (3/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 490...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 300.0% of the lines (3/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 500...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 510...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 300.0% of the lines (3/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 520...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 300.0% of the lines (3/1)\n",
            "WARNING:phonemizer:words count mismatch on 300.0% of the lines (3/1)\n",
            "WARNING:phonemizer:words count mismatch on 300.0% of the lines (3/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 530...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 300.0% of the lines (3/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 540...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 300.0% of the lines (3/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 300.0% of the lines (3/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 300.0% of the lines (3/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 300.0% of the lines (3/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 550...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 300.0% of the lines (3/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 300.0% of the lines (3/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 560...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 570...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 300.0% of the lines (3/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 300.0% of the lines (3/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 580...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 300.0% of the lines (3/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 590...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 600...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 300.0% of the lines (3/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 610...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 300.0% of the lines (3/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 300.0% of the lines (3/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 620...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 300.0% of the lines (3/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 630...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 300.0% of the lines (3/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 300.0% of the lines (3/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 640...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 300.0% of the lines (3/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 650...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 660...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 670...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 300.0% of the lines (3/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 680...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 300.0% of the lines (3/1)\n",
            "WARNING:phonemizer:words count mismatch on 300.0% of the lines (3/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 690...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 300.0% of the lines (3/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 300.0% of the lines (3/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 700...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 300.0% of the lines (3/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 300.0% of the lines (3/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 710...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 300.0% of the lines (3/1)\n",
            "WARNING:phonemizer:words count mismatch on 300.0% of the lines (3/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 300.0% of the lines (3/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 720...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 300.0% of the lines (3/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 730...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 300.0% of the lines (3/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 740...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 300.0% of the lines (3/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 750...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 300.0% of the lines (3/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 300.0% of the lines (3/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 300.0% of the lines (3/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 760...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 400.0% of the lines (4/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 770...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 300.0% of the lines (3/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 800...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 810...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 820...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 830...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 840...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 850...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 860...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 900...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 910...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 920...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 960...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 970...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved chunk 980...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Success! Saved to final_audiobook.wav\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kwJUxMueMDnA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uY99QUZ9MDpn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C512NABIMDsB"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}